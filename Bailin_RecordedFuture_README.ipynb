{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by: Emma Bailin\n",
    "\n",
    "Created on: October 15, 2019\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "Last Modified: October 31, 2019 --> addendum added\n",
=======
    "Last Modified: October 22, 2019\n",
>>>>>>> 315182e645bb7447bdc4375487f4aa133cd2b7bf
=======
    "Last Modified: October 22, 2019\n",
>>>>>>> 315182e645bb7447bdc4375487f4aa133cd2b7bf
    "\n",
    "Description: Identifies the duplicate Digital Certificates based on their leaf certificate fingerprint, and generate an output file with sets of duplicate input rows, one row per set of duplicates.\n",
    "\n",
    "### Disclaimer\n",
    "I have gone back through the code I originally wrote, called searchDuplicatesCrts.py, to create this for you. This is significantly cleaner (or at least better organized) than the original. That said, I'm assuming that you're just as interested in *how* I came to my solutions as you are in seeing *what* I did. Luckily for you, a huge portion of my thought process comes from simple train-of-thought writing, which I tend to do in block comments. While I usually erase the ramblings and print statements before I share my scripts, I decided not to for this exercise. To make it easier on you, however, I've gone through those train-of-thoughts and organized them chronologically and added, hopefully, clarifying commentary. You're welcome to skip them. If you see any typos, which you probably will, it's because there's no spellcheck in my script editor (SublimeText). I'll try to catch what I can here, but I'm definitely going to miss some. I apologize in advance.\n",
    "\n",
    "Additionally, if you feel brave, you can find all of my work, including my print-statment debugging, timing records, and less-successful avenues of pursuit, in the raw searchDuplicatesCrts.py. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Thoughts \n",
    "**ad-hoc**\n",
    "\n",
    "Pre-coding Notes:\n",
    "\t1) Output is a dictionary, so presumably, converting the JSON object into a dictionary\n",
    "\t2) Brute force ==> read in each json, compare to contents in dictionary, if not there, put in dict, otherwise, put json dict into hash of repeats for eventual return. Rinse. Repeat\n",
    "\t3) Known problem: We can't just compare dictionaries if there are lists inside the dictionary, the lists may be out of order and it'll miss repeats. Solution: Make sure the orders are the same. \n",
    "\t\tCould sort each column in dict before putting in bigger?\n",
    "\t\t--> Let's start there and see if we can improve later. \n",
    "\t\t**-->WAIT!!! NO! We only need to look at the fingerprints! The fingerprints aren't lists! So we don't actually care about the sorting!**\n",
    "\t4) PANDAS dataframe is better suited to huge datasets than dicts. It may be better to store the info in there. \n",
    "\n",
    "So The PLAN:\n",
    "\tjson=loads(file)\n",
    "\tdata=json[] \n",
    "\t#sort\n",
    "\tfor col in data:\n",
    "\t\tlook for any lists-->yes--> order --> repeat\n",
    "\t\t--> no --> continue\n",
    "\t#dump into data.frame [OR is it faster to append to list and then put list of dicts into dataframe? Yeah, that's faster, according to stackoverflow]\n",
    " \tdatas.append(data)\n",
    "\tfiles=pd.DataFrame(datas)\n",
    "\t#now we can do comparisons\n",
    "\t( What do you know! PANDAS has a function for that! it's just pd.DataFrame.duplicated. ) [Perhaps look at other methods and timeit?]\n",
    "\tdups=files.duplicated()\n",
    "\tdups.write_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ijson'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ce0dece2ddea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mijson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myajl2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mijson\u001b[0m  \u001b[0;31m# WAY faster for importing and working with jsons (https://pypi.org/project/ijson/)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmkdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ijson'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import timeit  # testing purposes\n",
    "import timing  # for complete script timings. Taken from an answer on stackoverflow years ago. NOT MINE\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import ijson.backends.yajl2 as ijson  # WAY faster for importing and working with jsons (https://pypi.org/project/ijson/)\n",
    "from glob import glob\n",
    "from os import path, getcwd, mkdir\n",
    "from pandas.io.json import json_normalize\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 1: usingJson\n",
    "**post-hoc**\n",
    "\n",
    "This is what I consider the 'brute' attack. It was my first thought. You'll see that I wrote a method called 'sortDictLists'. Only after running this a few did it hit me that I had no reason to sort the lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is never used. It is a relic of me thinking we needed to sort the lists for comparisons\n",
    "def sortDictLists(data):\n",
    "    if isinstance(data, dict):\n",
    "        return sorted((k, sortDictLists(v)) for k, v in data.items())\n",
    "    if isinstance(data, list):\n",
    "        return sorted(sortDictLists(x) for x in data)\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "# Noted copied from https://stackoverflow.com/questions/24448543/how-would-i-flatten-a-nested-dictionary-in-python-3\n",
    "def flattenDict(current, key, result):\n",
    "    if isinstance(current, dict):\n",
    "        for k in current:\n",
    "            new_key = \"{0}.{1}\".format(key, k) if len(key) > 0 else k\n",
    "            flattenDict(current[k], new_key, result)\n",
    "    else:\n",
    "        result[key] = current\n",
    "    return result\n",
    "\n",
    "\n",
    "def usingJson(file):\n",
    "    certs = []  # for holding all of the ordered data\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            cert = flattenDict(json.loads(line), \"\", {})  # , '', {})\n",
    "\n",
    "            # we need to put the strings from sortDict back into dicts.\n",
    "            # MAYBE THIS ISN'T NECESSARY?!\n",
    "            cs = {}\n",
    "            for c in cert:\n",
    "                cs.update({c[0]: c[1]})\n",
    "            # print(\"*****type: \", type(cert))\n",
    "            # print(\"cert: \",cert)\n",
    "            # certs.append(cs)\n",
    "            certs.append(cert)\n",
    "\n",
    "    certsDF = pd.DataFrame(certs)\n",
    "\n",
    "    dups = certsDF.duplicated(subset=\"data.leaf_cert.fingerprint\")\n",
    "    dups = certsDF[dups]\n",
    "    # print(dups)\n",
    "    # dups.to_csv(\"subsample_json_DUPS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 2: usingIjson\n",
    "**post-hoc**\n",
    "\n",
    "After getting a baseline of usingJson with the 82MB (ctl_records_subsample), 177MB (subsample2), and 996.5MB (subsample3) files, I went back to the internet, specifically looking for alternatives to my brute force. I found ijson.\n",
    "\n",
    "Unfortunately, ijson wasn't the magic I'd hoped for. I quickly found that it was much more useful for a list of json *files*, not *strings* inside one file. Like any good coder, I googled this issue until I found a solution: making new parsers for each line. That leads us to usingIjson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usingIjson(file):\n",
    "    certs = []  # for holding all of the ordered data\n",
    "\n",
    "    # from https://stackoverflow.com/questions/37200302/using-python-ijson-to-read-a-large-json-file-with-multiple-json-objects\n",
    "    with open(file, encoding=\"UTF-8\") as json_file:\n",
    "        for line_number, line in enumerate(json_file):\n",
    "            line_as_file = io.StringIO(line)\n",
    "            \n",
    "            # Use a new parser for each line\n",
    "            json_parser = ijson.parse(line_as_file)\n",
    "            cert = {}\n",
    "            for prefix, kind, value in json_parser:\n",
    "                if \"string\" == kind:\n",
    "                    cert.update({prefix: value})\n",
    "            certs.append(cert)\n",
    "\n",
    "    certsDF = pd.DataFrame(certs)\n",
    "    # print(\"dim(certsDF): \", certsDF.shape)\n",
    "    # print(\"data.columns: \", certsDF.columns)\n",
    "    # certsDF.to_csv(\"subsample_ijson_pd.csv\")\n",
    "\n",
    "    # print()\n",
    "    dups = certsDF.duplicated(subset=\"data.leaf_cert.fingerprint\")\n",
    "    dups = certsDF[dups]\n",
    "    # print(dups)\n",
    "    # dups.to_csv(\"subsample_ijson_DUPS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**post-hoc**\n",
    "\n",
    "With two methods written, I was ready to see which direction to continue down, straight json or ijson.\n",
    "\n",
    "I ran python's timeit function and found that usingJson outstripped usingIjson _considerably_. (~37 seconds to 8 seconds, on the smallest sample!)\n",
    "\n",
    "My choice was therefore easy. I was going to pursue json, and leave ijson alone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 3: usingJson_v2\n",
    "**ad-hoc**\n",
    "\n",
    "The problem is probably that I'm having to store everything in memory. But what if I don't need to? I just want to keep things in 'working' memory...\n",
    "\n",
    "**post-hoc**\n",
    "\n",
    "You'll notice that usingJson_v2 is nearly identical to usingJson. The only difference is that I'm creating a generator, not making a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usingJson_v2(file):\n",
    "    cs = []\n",
    "    with open(file, \"rb\") as f:\n",
    "        certs = (\n",
    "            json.loads(line) for line in f\n",
    "        )  # make generator, don't store in memory directly.\n",
    "\n",
    "        for cert in certs:\n",
    "            cert = flattenDict(cert, \"\", {})\n",
    "            cs.append(cert)\n",
    "\n",
    "    # certsDF=pd.DataFrame(certs)\n",
    "    certsDF = pd.DataFrame(cs)\n",
    "\n",
    "    # print(\"dim(certsDF): \", certsDF.shape)\n",
    "    # print(\"data.columns: \", certsDF.columns)\n",
    "    # certsDF.to_csv(\"subsample_ijson_pd.csv\")\n",
    "\n",
    "    # print()\n",
    "    dups = certsDF.duplicated(subset=\"data.leaf_cert.fingerprint\")\n",
    "    dups = certsDF[dups]\n",
    "    # dups.to_csv(\"duplicates.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**post-hoc**\n",
    "\n",
    "In hindsight, I understand that the increase in speed was negligible overall, finding that usingJson_v2 was only _barely_ faster than usingJson threw me for a loop. I decided to approach it from a completely different angle, which takes us to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usingPANDAS(file):\n",
    "    # set the size of the information to process at any time, so we're not sending it to memory\n",
    "    chunks = (\n",
    "        []\n",
    "    )  # for storing all the chunks so that we can send them all to a merged dataframe later.\n",
    "\n",
    "    # PANDAS has a jsonreader that works with chunks! AND it works well with line-delimitors\n",
    "    timing.log(\"Starting reading-in\")\n",
    "    reader = pd.read_json(\n",
    "        (file), lines=True, chunksize=100000, dtype=False\n",
    "    )  # so it doesn't infer the type?\n",
    "    timing.log(\"Starting chunk processing\")\n",
    "    for chunk in reader:\n",
    "        new = chunk[\"data\"].apply(json.dumps)\n",
    "        new = json_normalize(new.apply(json.loads))\n",
    "        \n",
    "        for column in chunk.columns:\n",
    "            if \"data\" != column:\n",
    "                new[column] = chunk[column]\n",
    "        del new[\"data\"]  # remove data now because dictionary screws things up later.\n",
    "        \n",
    "        chunks.append(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ad-hoc**\n",
    "\n",
    "\n",
    "[FORMER IN-LINE BLOCK-COMMENT]\n",
    "\n",
    "NEW plan. Clearly the chunking is a good idea, but we still have to deal with the json strings. So let's try to normalize those AFTER. If the column contains strings that are valid jsons, convert them.\n",
    "\tNope. Not working. We can't treat them like jsons, because they're going in as dictionaries. They're not acting like dictionaries, though. The data column is a series. I want to be able to extract the dictionaries and then attach them to the rest of the dataframe, so we preserve any columns that aren't the 'data'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "2019-10-21 20:57:18 - Starting Concat\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9b2d06c06845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# now make a major dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtiming\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting Concat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcertsDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtiming\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finding dups\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunks' is not defined"
     ]
    }
   ],
   "source": [
    "    # now make a major dataframe\n",
    "    timing.log(\"Starting Concat\")\n",
    "    certsDF = pd.concat(chunks, ignore_index=True, sort=True)\n",
    "    \n",
    "    timing.log(\"finding dups\")\n",
    "    dups = certsDF.duplicated(subset=\"leaf_cert.fingerprint\")\n",
    "    dups = certsDF[dups]\n",
    "    timing.log(\"writing dups to file\")\n",
    "    dups.to_csv(\"duplicates_PANDAS.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-brainstorming! \n",
    "\n",
    "**ad-hoc**\n",
    "\n",
    "I was on to something with the usingPANDAS. With that one, I found that the reading in was NOT the problem, it was the calculations. I also know that for small chunks, the calculations can be done much faster. So why not combine that?\n",
    "\t1) Read in with chunks like in usingPANDAS\n",
    "\t2) For each chunk, run the process, THEN WRITE OUT TO A PANDAS csv\n",
    "\t3) At the end, read in all of the csvs. (Or maybe just append to one csv )\n",
    "\t4) Find the duplicates\n",
    "\t5) DONE\n",
    "\n",
    "\n",
    "I could find the duplicates in each chunk, and in fact, that was my first thought, but that assumes that the json strings are sorted in a way that all of the repeats are near each other. More likely, the jsons are sorted by time or something, so there's no promise that I'll find all of the repeats. Better to just send out the whole chunk to file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**post-hoc**\n",
    "\n",
    "As you can see, usingPANDAS wasn't as intuitive as the previous methods. I'd found json_normalize, but I spent an inordinate amount of time trying to get it to work with me (hence the in-block comment). Finally, it hit me: convert the contents of 'data' BACK into a json string! To my shock, it worked. \n",
    "\n",
    "I spent some time tuning the chunk size and using timeit again, I found that usingPANDAS worked _much_ better either of the usingJson methods. \n",
    "\n",
    "It was at this time that I emailed you asking for your definition of 'reasonable' time. Your response of \"1-2 hours\" left me gaping. Despite usingPANDAS big success, when I attempted to run the 31GB file, I had to cancel the program after over four hours. So I went back to the drawing board. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 4: readInOutIn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readInOutIn(file):\n",
    "    chunks = []\n",
    "    folder = path.join(getcwd(), \"temp\")\n",
    "    if not path.isdir(folder):\n",
    "        mkdir(folder)\n",
    "\n",
    "    timing.log(\"Starting reading-in\")\n",
    "    reader = pd.read_json((file), lines=True, chunksize=100000, dtype=False)\n",
    "\n",
    "    timing.log(\"Starting chunk processing\")\n",
    "    label = (\n",
    "        0\n",
    "    )  # for keeping track of the files. Could just do enumerate, but why get the length of reader? It could be huge.\n",
    "    for chunk in reader:\n",
    "        new = chunk[\"data\"].apply(json.dumps)\n",
    "        new = json_normalize(new.apply(json.loads))\n",
    "\n",
    "        for column in chunk.columns:\n",
    "            if \"data\" != column:\n",
    "                new[column] = chunk[column]\n",
    "        del new[\"data\"]  # remove data now because dictionary screws things up later.\n",
    "\n",
    "        # NOW WRITE OUT!\n",
    "        timing.log(\"Writing csv chunk %s\" % label)\n",
    "        new.to_csv(path.join(folder, \"chunk_%s.csv\" % label), chunksize=100000)\n",
    "        label += 1\n",
    "\n",
    "    timing.log(\"Reading back in!\")\n",
    "    files = glob(path.join(folder, \"*.csv\"))\n",
    "    for f in files:\n",
    "        new = pd.read_csv(file)\n",
    "        chunks.append(new)\n",
    "\n",
    "    # now convert the list of dataframes certsDF=pd.concat(chunks, ignore_index=True, sort=True)into a single dataframe\n",
    "    certsDF = pd.concat(chunks, ignore_index=True, sort=True)\n",
    "\n",
    "    timing.log(\"finding dups\")\n",
    "    dups = certsDF.duplicated(subset=\"leaf_cert.fingerprint\")\n",
    "    dups = certsDF[dups]\n",
    "    timing.log(\"writing dups to file\")\n",
    "    dups.to_csv(\"duplicates_readInOutIn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**post-hoc**\n",
    "\n",
    "I seriously considered not including this method, because I never actually tested it. As I was preparing to do so, it hit me that what I was trying to do was break the file into even smaller pieces than the chunks, but the size of the file wasn't really the issue. The problem stemmed from the amount of time it takes to _process_ each row.\n",
    "\n",
    "**ad-hoc**\n",
    "\n",
    "As I was thinking about getting readInOutIn to work, it hit me that I've been thinking about this a consecutive manner. There's only so far I can go with it. I need to think about serializing, ergo, THREADING! \n",
    "\n",
    "I know that reading in a file shouldn't be threaded, but given that that's not the part that takes forever (based on my previous attempts). That part is the processing the chunks (i.e. the normalizing) portion. So I can thread that. \n",
    "\n",
    "Just as I was about to add threading to my usingPANDAS framework, I decided to google adding threading to PANDAS. This lead to DASK! Rather than reinventing the wheel, I'm using it. https://docs.dask.org/en/latest/dataframe-api.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 5: usingDASK\n",
    "**post-hoc**\n",
    "\n",
    "The code that follows is so small comparied to the amount of time I spent trying to get it to work. There were a number of road-blocks, including having to define the column types (eventually I gave up being flexible and created the \"metas\" dictionary, which I didn't end up using), the nested jsons (again!) (I suddenly remembered my flattenDict method from days ago), and tuning the blocksize. I did have all of these adventures outlined, but eventually I found myself going in circles and spinning in the mud. I wiped the slate clean and started from scratch. As I'm sure you've experienced, I solved the problems within the hour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def usingDASK(file):\n",
    "    chunks = []\n",
    "    timing.log(\"Starting reading-in\")\n",
    "\n",
    "    reader = dd.read_json(\n",
    "        file,\n",
    "        lines=True,\n",
    "        blocksize=2 ** 24, #blocksize tuned using 1GB sample file\n",
    "        meta={\"data\": object, \"message_type\": object},\n",
    "    )\n",
    "  \n",
    "    datas = (\n",
    "        reader[\"data\"]\n",
    "        .map_partitions(lambda df: df.apply((lambda row: flattenDict(row, \"\", {}))))\n",
    "        .to_bag()\n",
    "    )\n",
    "    new = datas.to_dataframe()\n",
    "    new[\"message_type\"] = reader[\"message_type\"]\n",
    "    new = new.compute()\n",
    "    dups = new.duplicated(subset=\"leaf_cert.fingerprint\")\n",
    "    dups = new[dups]\n",
    "    dups.to_csv(\"duplicates_DASK.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "My poor laptop is still running the huge file with usingDASK, but my preliminary testing suggests that it is as much of an improvement in speed as I hope. There are still 30 minutes before it hits the 2 hour mark, but frankly, even if it goes over, it's 8:30pm the day before I have to give this to you. I'm not holding my breath that I'll be able to improve on the final time.\n",
    "\n",
    "If you were wondering, I did begin the bonus challenge while I was waiting for things to process. I didn't get anywhere near as far as I wanted, but if you're at all curious about my initial plan of attack, the script is 'bonusFindPhishing.py'. It gives a more realistic view of how my scripts are typically organized and commented than searchDuplicateCrts.py.\n",
    "\n",
    "I want to thank you for giving me this chance and for reading this to the end. I hope you find what you're looking for in me. If you have any questions, you know how to reach me.\n",
    "\n",
    "Sincerely,\n",
    "\n",
    "Emma Bailin\n",
    "\n",
    "Update (morning before submission): usingDASK wasn't as successful as I'd hoped. I'm suspicious that the blocksize I chose for usingDASK was too big. I tuned it using the 1GB file, but I don't think I took the expansion into account. Baring the blocksize tuning, I can see another avenue being using sqlite or something similar to do the loading, leveraging Dask's prep expertise and SQL's storage options. I'd like to explore these options, but I recognize I need to submit what I've got to you now. "
   ]
<<<<<<< HEAD
<<<<<<< HEAD
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADDENDUM: 9 days later...\n",
    "\n",
    "As I said above,  I did continue to work on this and as of yesterday afternoon, I solved half of the problem. I managed to load the entire ~31GB file into a MySQL database in a little under 2 hours. Unfortunately, MySQL had a bit of trouble doing the duplicates (or, as it has just occured to me, I shouldn't have combined Dask and the SQL query?). It took a little over 20 hours to find the duplicates and to write out the new csv. As I did above, I have copied my reasoning and added commentary. The \"dirty\" code can be seen in the github previously given (https://github.com/ebailin/RecordedFutureChallenge.git), in the file \"afterRFSub_searchDuplicatesCrts.py\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: usingDASKClient\n",
    "\n",
    "**post-hoc**\n",
    "\n",
    "After I submitted the orginal file, I took a day and then went back to research. Knowing that Dask had gotten me the closest, I went back to the API. The following is the \"real-time\" considerations:\n",
    "\n",
    "**ad-hoc**\n",
    "When we last left off, I was still trying to get dask to work. I've since concluded that my setup was good, but the compute() method was the holdup. I thought there was no solution to the compute issue,but then I found Dask Client! Having watched the progress monitor on the linux, I've seen that it doesn't seem like Dask is using all of the CPU or the cores the computer has available. Maybe if I force it, it'll actually do the computations?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usingDASKClient(file):\n",
    "\tclient=Client(\n",
    "        memory_limit='5GB', \n",
    "        processes=True, \n",
    "        threads_per_worker=4, \n",
    "        n_workers=1\n",
    "    ) #so we can use distributed\n",
    "\tclient.restart()\n",
    "\tchunks = []\n",
    "\ttiming.log(\"Starting reading-in\")\n",
    "\n",
    "\treader = dd.read_json(\n",
    "        file,\n",
    "        lines=True,\n",
    "        blocksize=2 ** 28,\n",
    "        meta={\"data\": object, \"message_type\": object},\n",
    "    )\n",
    "    \n",
    "\tdatas = (\n",
    "        reader[\"data\"]\n",
    "        .map_partitions(\n",
    "            lambda df: df.apply(\n",
    "                (lambda row: flattenDict(row, \"\", {}))\n",
    "            ))\n",
    "        .to_bag()\n",
    "    )\n",
    "    \n",
    "\tnew = datas.to_dataframe()\n",
    "\tnew[\"message_type\"] = reader[\"message_type\"]\n",
    "\n",
    "\ttiming.log(\"Starting compute\")\n",
    "\tnew = new.compute()\n",
    "\tdups = new.duplicated(subset=\"leaf_cert.fingerprint\")\n",
    "\tdups = new[dups]\n",
    "    dups.to_csv(\"duplicates_DASK.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**post-hoc**\n",
    "\n",
    "To my astonishment (and disappointment), using the Dask Client actually backfired. The 1GB file actually ended up freezing my computer for over three hours before I finally managed to kill the process. Now that I've gone over it a few dozen times or so, I think it's because, even though I tried to get my linux to 'fake' being a cluster, it just couldn't do it. No matter how many times and ways I configured the client. So I went with my hail mary plan: the SQL method. \n",
    "\n",
    "I'd considered SQL as an avenue a few times, but I'd dismissed the ideas almost as soon as they formed because I was afraid of all of the writing in and out of memory. I'd heard of bulk memory inserts, but surely for something as _huge_ as the file I was dealing with, getting down from a week to a couple of days wasn't worth it? After all, I reasoned, it still has to read the whole json and then divide it into chunks to upload to SQL. Even when I found Dask, it didn't occur to me that I could use it until two hours before I submitted the original markdown. After the failure that was usingDASKClient, however, I thought, \"it can't be any worse\". The following is my in-the-moment reasoning. \n",
    "\n",
    "\n",
    "**ad-hoc**\n",
    "\n",
    "That's a no-go for the client. It keeps running out of memory. I don't think this computer has enough spare memory to split. So let's move to a completely different foot. Let's try mysql loading.\n",
    "\n",
    "[...]\n",
    "\n",
    "With this attempt, I'm going to try to read everything in with dask and send it chunk-wise into MySQL.\n",
    "\n",
    "After attempting to use MySQL and running into an issue inserting chunks, I'm going to follow the tutorial here: https://towardsdatascience.com/how-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55. I wasn't going to use the copy_from, but it seems like it's a good idea given that we know the data is already validated and we just want to shove it in. Unfortunately, the copy_from method is only from pyscopg2, which means I'll need to switch to postgresSQL. \n",
    "\tOR\n",
    "\tI could just use sqlalchemy's sessions and bulk insert? Let's try that first so  I don't have to switch to postgresSQL.--> Not worth it. The session method, while nice, requires that we have a pre-determined base, i.e. a table, already in the database. We don't. UNLESS, perhaps I use the first method to create the table, take the mapping, then use it for the rest? Is that worth it? Let's see. \n",
    "    \n",
    "**post-hoc**\n",
    "\n",
    "As you'll see below, I did try the session method and it worked, so I didn't end up fully implementing the other ideas (though I did get pretty far in a couple of cases--see the dirty code). If you're wondering why I didn't just use postgresSQL from the start, as the tutorial and just about every other posting about this technique on Google did, my only excuse is that I've already got a mysql server and client running. I didn't want to have to configure another server/client if it weren't absolutely necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usingMySQL(file):\n",
    "\ttiming.log(\"Setting Up MySQL\")\n",
    "\t#MySQL setup\n",
    "\t#I probably should do this with another method and a config file, but this is a test anyway. I'm breaking protocol just to get it done. \n",
    "\tinfo=pd.read_csv(path.join(getcwd(), \"private.csv\"))\n",
    "\tengine=sql.create_engine(\n",
    "        'mysql+pymysql://{0}:{1}@{2}/{3}?charset=utf8mb4'\n",
    "        .format(\n",
    "            info['user'][0], \n",
    "            info['password'][0], \n",
    "            info['host'][0], \n",
    "            info['database'][0]\n",
    "        )\n",
    "    ) #maybe try with fast_executemany if need to use pyodbc?\n",
    "\n",
    "\tconn=engine.connect()\n",
    "\terrored=[] #for tables that don't cooperate\n",
    "\n",
    "\ttiming.log(\"Starting read-in\")\n",
    "\treader = dd.read_json(\n",
    "        file,\n",
    "        lines=True,\n",
    "        blocksize=2 ** 28,\n",
    "        meta={\"data\": object, \"message_type\": object},\n",
    "    )\n",
    "\tdatas = (\n",
    "        reader[\"data\"]\n",
    "        .map_partitions(lambda df: df.apply((lambda row: flattenDict(row, \"\", {}))))\n",
    "        .to_bag()\n",
    "    )\n",
    "\ttiming.log(\"Making new\")\n",
    "\tnew = datas.to_dataframe().fillna(value=\"None\") #for padding later during the insert\n",
    "\n",
    "    #added after realized issue with the chunk to sql. See note there. [**Note removed and put in commentary**]\n",
    "\ttiming.log(\"Changing dtypes in new\")\n",
    "\tfor k,v in dict(new.dtypes).items():\n",
    "\t\tif v != \"object\":\n",
    "\t\t\tnew[k]=new[k].astype(str)\n",
    "\tnew[\"message_type\"] = reader[\"message_type\"]\n",
    "\n",
    "\ttiming.log(\"Making table in db\")\n",
    "\t#make empty dataframe for creating an empty table\n",
    "\tpd.DataFrame(columns=new.columns).to_sql('crts', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "\t#need to set primary key for the mapping:\n",
    "\twith engine.connect() as con:\n",
    "\t\tcon.execute(\n",
    "            'ALTER TABLE crts ADD id SERIAL PRIMARY KEY;'\n",
    "        )\n",
    "\n",
    "\ttiming.log(\"Making mappings\")\n",
    "\t#now get the mapping from the table created above?\n",
    "\tmeta=MetaData(bind=engine)\n",
    "\tmeta.reflect(engine)\n",
    "\tBase=automap_base(metadata=meta)\n",
    "\tBase.prepare()\n",
    "\tCert=Base.classes.crts #the cert class\n",
    "\n",
    "\n",
    "\t#Post session-maker addition.\n",
    "\tSession=sessionmaker(bind=conn)\n",
    "\tsession=Session()\n",
    "\ttiming.log(\"starting bulk loading!\")\n",
    "\tfor n in range(new.npartitions):\n",
    "\t\tchunk=new.get_partition(n).applymap(str).compute()\n",
    "\t\toutput=StringIO()\n",
    "\t\tsession=Session()\n",
    "\t\tsession.bulk_insert_mappings(Cert, \n",
    "                                     chunk.to_dict(\n",
    "                                         orient=\"records\")\n",
    "                                    )\n",
    "\t\tsession.commit()\n",
    "\t\tsession.close()\n",
    "\n",
    "\ttiming.log(\"Starting the sql computations\")\n",
    "\t#now do the computation!\n",
    "\tquery=\"SELECT cert1.* \\\n",
    "            FROM crts \\\n",
    "            AS cert1 \\\n",
    "            JOIN (\\\n",
    "                SELECT `leaf_cert.fingerprint` \\\n",
    "                FROM crts \\\n",
    "                GROUP BY `leaf_cert.fingerprint` \\\n",
    "                HAVING COUNT(`leaf_cert.fingerprint`) > 1) \\\n",
    "                AS cert2 \\\n",
    "            ON \\\n",
    "                cert1.`leaf_cert.fingerprint` = cert2.`leaf_cert.fingerprint`\")\n",
    "\n",
    "\t#read straight from MySQL into new pandas for fast dumping to csv (and we hope that it's small enough for memory...)\n",
    "\tdups=pd.read_sql(query, engine)\n",
    "\ttiming.log(\"Dumping excess duplicates (JUST IN CASE)\")\n",
    "\t\n",
    "    dups=dups[dups.duplicated(subset=\"leaf_cert.fingerprint\")] #ADDED THE DUPLICATED CHECK B/C ASSUMING that we only want one example\n",
    "\ttiming.log(\"Writing out!\")\n",
    "\t\n",
    "    dups.to_csv(path.join(getcwd(), \"usingMySQL_dups.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ad-hoc** \n",
    "[FORMER IN-LINE BLOCK-COMMENT]\n",
    "\n",
    "LOOKING at the data, it appears that the 'seen' column USUALLY contains a number, but sometimes something gets off and then it contains a string. With this in mind, I should only need to make the seen column a string, rather than everything. This would also explain why the 'head()' gets inserted just fine, but not the whole thing.\n",
    "\n",
    "ACTUALLY, it appears that the issue comes from any column that's labeled as not a string! In a few rows, it seems that all of the data is offset by one or two. This is something wrong with the initial json, which I need to  check later. For now, if the column type isn't a string, make it one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**post-hoc**\n",
    "\n",
    "Again, to my dismay, I was disappointed, a timeit of the 100MB file calculated that it took a whopping 43.5 seconds to compute! I was about to ditch the whole method and start again when I noticed that for the first time, timeit had run 10 loops instead of the one as I'd come to expect. On a hunch, I ran the 1GB using timeit, but it had a memory crash after what seemed to be three renditions (not loop). I decided to just use my Timing module to run it once. It took 4 minutes and 44 seconds. \n",
    "\n",
    "**ad-hoc**\n",
    "\n",
    "Got a memory block overload trying to do timeit with the 1GB. I'm really hoping that's just because of the number of times timeit ran? \n",
    "\tLooking at just my time function, it took 4 minutes and 44 seconds to do the 1GB. It's official that usingMySQL is the slowest method so far for the little files, BUT, the methods that were very fast for the subfiles crashed the big one, so maybe it's worth running it on the 31GB? It's late enough that I'll just let it run. If it crashes, I tried. \n",
    "  \n",
    "## Conclusion 2\n",
    "That brings us back to the start of this addendum. Now that I've got the data into MySQL, and knowing that I may be able to speed the loading by increasing the size of the chunks or some other minor tweaking, I can focus on getting the SQL computations faster. But that'll be a story for another day..."
   ]
=======
>>>>>>> 315182e645bb7447bdc4375487f4aa133cd2b7bf
=======
>>>>>>> 315182e645bb7447bdc4375487f4aa133cd2b7bf
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
