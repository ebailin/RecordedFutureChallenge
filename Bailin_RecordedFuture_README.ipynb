{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by: Emma Bailin\n",
    "\n",
    "Created on: October 15, 2019\n",
    "\n",
    "Last Modified: October 22, 2019\n",
    "\n",
    "Description: Identifies the duplicate Digital Certificates based on their leaf certificate fingerprint, and generate an output file with sets of duplicate input rows, one row per set of duplicates.\n",
    "\n",
    "### Disclaimer\n",
    "I have gone back through the code I originally wrote, called searchDuplicatesCrts.py, to create this for you. This is significantly cleaner (or at least better organized) than the original. That said, I'm assuming that you're just as interested in *how* I came to my solutions as you are in seeing *what* I did. Luckily for you, a huge portion of my thought process comes from simple train-of-thought writing, which I tend to do in block comments. While I usually erase the ramblings and print statements before I share my scripts, I decided not to for this exercise. To make it easier on you, however, I've gone through those train-of-thoughts and organized them chronologically and added, hopefully, clarifying commentary. You're welcome to skip them. If you see any typos, which you probably will, it's because there's no spellcheck in my script editor (SublimeText). I'll try to catch what I can here, but I'm definitely going to miss some. I apologize in advance.\n",
    "\n",
    "Additionally, if you feel brave, you can find all of my work, including my print-statment debugging, timing records, and less-successful avenues of pursuit, in the raw searchDuplicatesCrts.py. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Thoughts \n",
    "**ad-hoc**\n",
    "\n",
    "Pre-coding Notes:\n",
    "\t1) Output is a dictionary, so presumably, converting the JSON object into a dictionary\n",
    "\t2) Brute force ==> read in each json, compare to contents in dictionary, if not there, put in dict, otherwise, put json dict into hash of repeats for eventual return. Rinse. Repeat\n",
    "\t3) Known problem: We can't just compare dictionaries if there are lists inside the dictionary, the lists may be out of order and it'll miss repeats. Solution: Make sure the orders are the same. \n",
    "\t\tCould sort each column in dict before putting in bigger?\n",
    "\t\t--> Let's start there and see if we can improve later. \n",
    "\t\t**-->WAIT!!! NO! We only need to look at the fingerprints! The fingerprints aren't lists! So we don't actually care about the sorting!**\n",
    "\t4) PANDAS dataframe is better suited to huge datasets than dicts. It may be better to store the info in there. \n",
    "\n",
    "So The PLAN:\n",
    "\tjson=loads(file)\n",
    "\tdata=json[] \n",
    "\t#sort\n",
    "\tfor col in data:\n",
    "\t\tlook for any lists-->yes--> order --> repeat\n",
    "\t\t--> no --> continue\n",
    "\t#dump into data.frame [OR is it faster to append to list and then put list of dicts into dataframe? Yeah, that's faster, according to stackoverflow]\n",
    " \tdatas.append(data)\n",
    "\tfiles=pd.DataFrame(datas)\n",
    "\t#now we can do comparisons\n",
    "\t( What do you know! PANDAS has a function for that! it's just pd.DataFrame.duplicated. ) [Perhaps look at other methods and timeit?]\n",
    "\tdups=files.duplicated()\n",
    "\tdups.write_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ijson'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ce0dece2ddea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mijson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myajl2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mijson\u001b[0m  \u001b[0;31m# WAY faster for importing and working with jsons (https://pypi.org/project/ijson/)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmkdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ijson'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import timeit  # testing purposes\n",
    "import timing  # for complete script timings. Taken from an answer on stackoverflow years ago. NOT MINE\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import ijson.backends.yajl2 as ijson  # WAY faster for importing and working with jsons (https://pypi.org/project/ijson/)\n",
    "from glob import glob\n",
    "from os import path, getcwd, mkdir\n",
    "from pandas.io.json import json_normalize\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 1: usingJson\n",
    "**post-hoc**\n",
    "\n",
    "This is what I consider the 'brute' attack. It was my first thought. You'll see that I wrote a method called 'sortDictLists'. Only after running this a few did it hit me that I had no reason to sort the lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is never used. It is a relic of me thinking we needed to sort the lists for comparisons\n",
    "def sortDictLists(data):\n",
    "    if isinstance(data, dict):\n",
    "        return sorted((k, sortDictLists(v)) for k, v in data.items())\n",
    "    if isinstance(data, list):\n",
    "        return sorted(sortDictLists(x) for x in data)\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "# Noted copied from https://stackoverflow.com/questions/24448543/how-would-i-flatten-a-nested-dictionary-in-python-3\n",
    "def flattenDict(current, key, result):\n",
    "    if isinstance(current, dict):\n",
    "        for k in current:\n",
    "            new_key = \"{0}.{1}\".format(key, k) if len(key) > 0 else k\n",
    "            flattenDict(current[k], new_key, result)\n",
    "    else:\n",
    "        result[key] = current\n",
    "    return result\n",
    "\n",
    "\n",
    "def usingJson(file):\n",
    "    certs = []  # for holding all of the ordered data\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            cert = flattenDict(json.loads(line), \"\", {})  # , '', {})\n",
    "\n",
    "            # we need to put the strings from sortDict back into dicts.\n",
    "            # MAYBE THIS ISN'T NECESSARY?!\n",
    "            cs = {}\n",
    "            for c in cert:\n",
    "                cs.update({c[0]: c[1]})\n",
    "            # print(\"*****type: \", type(cert))\n",
    "            # print(\"cert: \",cert)\n",
    "            # certs.append(cs)\n",
    "            certs.append(cert)\n",
    "\n",
    "    certsDF = pd.DataFrame(certs)\n",
    "\n",
    "    dups = certsDF.duplicated(subset=\"data.leaf_cert.fingerprint\")\n",
    "    dups = certsDF[dups]\n",
    "    # print(dups)\n",
    "    # dups.to_csv(\"subsample_json_DUPS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 2: usingIjson\n",
    "**post-hoc**\n",
    "\n",
    "After getting a baseline of usingJson with the 82MB (ctl_records_subsample), 177MB (subsample2), and 996.5MB (subsample3) files, I went back to the internet, specifically looking for alternatives to my brute force. I found ijson.\n",
    "\n",
    "Unfortunately, ijson wasn't the magic I'd hoped for. I quickly found that it was much more useful for a list of json *files*, not *strings* inside one file. Like any good coder, I googled this issue until I found a solution: making new parsers for each line. That leads us to usingIjson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usingIjson(file):\n",
    "    certs = []  # for holding all of the ordered data\n",
    "\n",
    "    # from https://stackoverflow.com/questions/37200302/using-python-ijson-to-read-a-large-json-file-with-multiple-json-objects\n",
    "    with open(file, encoding=\"UTF-8\") as json_file:\n",
    "        for line_number, line in enumerate(json_file):\n",
    "            line_as_file = io.StringIO(line)\n",
    "            \n",
    "            # Use a new parser for each line\n",
    "            json_parser = ijson.parse(line_as_file)\n",
    "            cert = {}\n",
    "            for prefix, kind, value in json_parser:\n",
    "                if \"string\" == kind:\n",
    "                    cert.update({prefix: value})\n",
    "            certs.append(cert)\n",
    "\n",
    "    certsDF = pd.DataFrame(certs)\n",
    "    # print(\"dim(certsDF): \", certsDF.shape)\n",
    "    # print(\"data.columns: \", certsDF.columns)\n",
    "    # certsDF.to_csv(\"subsample_ijson_pd.csv\")\n",
    "\n",
    "    # print()\n",
    "    dups = certsDF.duplicated(subset=\"data.leaf_cert.fingerprint\")\n",
    "    dups = certsDF[dups]\n",
    "    # print(dups)\n",
    "    # dups.to_csv(\"subsample_ijson_DUPS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**post-hoc**\n",
    "\n",
    "With two methods written, I was ready to see which direction to continue down, straight json or ijson.\n",
    "\n",
    "I ran python's timeit function and found that usingJson outstripped usingIjson _considerably_. (~37 seconds to 8 seconds, on the smallest sample!)\n",
    "\n",
    "My choice was therefore easy. I was going to pursue json, and leave ijson alone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 3: usingJson_v2\n",
    "**ad-hoc**\n",
    "\n",
    "The problem is probably that I'm having to store everything in memory. But what if I don't need to? I just want to keep things in 'working' memory...\n",
    "\n",
    "**post-hoc**\n",
    "\n",
    "You'll notice that usingJson_v2 is nearly identical to usingJson. The only difference is that I'm creating a generator, not making a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usingJson_v2(file):\n",
    "    cs = []\n",
    "    with open(file, \"rb\") as f:\n",
    "        certs = (\n",
    "            json.loads(line) for line in f\n",
    "        )  # make generator, don't store in memory directly.\n",
    "\n",
    "        for cert in certs:\n",
    "            cert = flattenDict(cert, \"\", {})\n",
    "            cs.append(cert)\n",
    "\n",
    "    # certsDF=pd.DataFrame(certs)\n",
    "    certsDF = pd.DataFrame(cs)\n",
    "\n",
    "    # print(\"dim(certsDF): \", certsDF.shape)\n",
    "    # print(\"data.columns: \", certsDF.columns)\n",
    "    # certsDF.to_csv(\"subsample_ijson_pd.csv\")\n",
    "\n",
    "    # print()\n",
    "    dups = certsDF.duplicated(subset=\"data.leaf_cert.fingerprint\")\n",
    "    dups = certsDF[dups]\n",
    "    # dups.to_csv(\"duplicates.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**post-hoc**\n",
    "\n",
    "In hindsight, I understand that the increase in speed was negligible overall, finding that usingJson_v2 was only _barely_ faster than usingJson threw me for a loop. I decided to approach it from a completely different angle, which takes us to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usingPANDAS(file):\n",
    "    # set the size of the information to process at any time, so we're not sending it to memory\n",
    "    chunks = (\n",
    "        []\n",
    "    )  # for storing all the chunks so that we can send them all to a merged dataframe later.\n",
    "\n",
    "    # PANDAS has a jsonreader that works with chunks! AND it works well with line-delimitors\n",
    "    timing.log(\"Starting reading-in\")\n",
    "    reader = pd.read_json(\n",
    "        (file), lines=True, chunksize=100000, dtype=False\n",
    "    )  # so it doesn't infer the type?\n",
    "    timing.log(\"Starting chunk processing\")\n",
    "    for chunk in reader:\n",
    "        new = chunk[\"data\"].apply(json.dumps)\n",
    "        new = json_normalize(new.apply(json.loads))\n",
    "        \n",
    "        for column in chunk.columns:\n",
    "            if \"data\" != column:\n",
    "                new[column] = chunk[column]\n",
    "        del new[\"data\"]  # remove data now because dictionary screws things up later.\n",
    "        \n",
    "        chunks.append(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ad-hoc**\n",
    "\n",
    "\n",
    "[FORMER IN-LINE BLOCK-COMMENT]\n",
    "\n",
    "NEW plan. Clearly the chunking is a good idea, but we still have to deal with the json strings. So let's try to normalize those AFTER. If the column contains strings that are valid jsons, convert them.\n",
    "\tNope. Not working. We can't treat them like jsons, because they're going in as dictionaries. They're not acting like dictionaries, though. The data column is a series. I want to be able to extract the dictionaries and then attach them to the rest of the dataframe, so we preserve any columns that aren't the 'data'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "2019-10-21 20:57:18 - Starting Concat\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9b2d06c06845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# now make a major dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtiming\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting Concat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcertsDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtiming\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finding dups\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunks' is not defined"
     ]
    }
   ],
   "source": [
    "    # now make a major dataframe\n",
    "    timing.log(\"Starting Concat\")\n",
    "    certsDF = pd.concat(chunks, ignore_index=True, sort=True)\n",
    "    \n",
    "    timing.log(\"finding dups\")\n",
    "    dups = certsDF.duplicated(subset=\"leaf_cert.fingerprint\")\n",
    "    dups = certsDF[dups]\n",
    "    timing.log(\"writing dups to file\")\n",
    "    dups.to_csv(\"duplicates_PANDAS.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-brainstorming! \n",
    "\n",
    "**ad-hoc**\n",
    "\n",
    "I was on to something with the usingPANDAS. With that one, I found that the reading in was NOT the problem, it was the calculations. I also know that for small chunks, the calculations can be done much faster. So why not combine that?\n",
    "\t1) Read in with chunks like in usingPANDAS\n",
    "\t2) For each chunk, run the process, THEN WRITE OUT TO A PANDAS csv\n",
    "\t3) At the end, read in all of the csvs. (Or maybe just append to one csv )\n",
    "\t4) Find the duplicates\n",
    "\t5) DONE\n",
    "\n",
    "\n",
    "I could find the duplicates in each chunk, and in fact, that was my first thought, but that assumes that the json strings are sorted in a way that all of the repeats are near each other. More likely, the jsons are sorted by time or something, so there's no promise that I'll find all of the repeats. Better to just send out the whole chunk to file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**post-hoc**\n",
    "\n",
    "As you can see, usingPANDAS wasn't as intuitive as the previous methods. I'd found json_normalize, but I spent an inordinate amount of time trying to get it to work with me (hence the in-block comment). Finally, it hit me: convert the contents of 'data' BACK into a json string! To my shock, it worked. \n",
    "\n",
    "I spent some time tuning the chunk size and using timeit again, I found that usingPANDAS worked _much_ better either of the usingJson methods. \n",
    "\n",
    "It was at this time that I emailed you asking for your definition of 'reasonable' time. Your response of \"1-2 hours\" left me gaping. Despite usingPANDAS big success, when I attempted to run the 31GB file, I had to cancel the program after over four hours. So I went back to the drawing board. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 4: readInOutIn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readInOutIn(file):\n",
    "    chunks = []\n",
    "    folder = path.join(getcwd(), \"temp\")\n",
    "    if not path.isdir(folder):\n",
    "        mkdir(folder)\n",
    "\n",
    "    timing.log(\"Starting reading-in\")\n",
    "    reader = pd.read_json((file), lines=True, chunksize=100000, dtype=False)\n",
    "\n",
    "    timing.log(\"Starting chunk processing\")\n",
    "    label = (\n",
    "        0\n",
    "    )  # for keeping track of the files. Could just do enumerate, but why get the length of reader? It could be huge.\n",
    "    for chunk in reader:\n",
    "        new = chunk[\"data\"].apply(json.dumps)\n",
    "        new = json_normalize(new.apply(json.loads))\n",
    "\n",
    "        for column in chunk.columns:\n",
    "            if \"data\" != column:\n",
    "                new[column] = chunk[column]\n",
    "        del new[\"data\"]  # remove data now because dictionary screws things up later.\n",
    "\n",
    "        # NOW WRITE OUT!\n",
    "        timing.log(\"Writing csv chunk %s\" % label)\n",
    "        new.to_csv(path.join(folder, \"chunk_%s.csv\" % label), chunksize=100000)\n",
    "        label += 1\n",
    "\n",
    "    timing.log(\"Reading back in!\")\n",
    "    files = glob(path.join(folder, \"*.csv\"))\n",
    "    for f in files:\n",
    "        new = pd.read_csv(file)\n",
    "        chunks.append(new)\n",
    "\n",
    "    # now convert the list of dataframes certsDF=pd.concat(chunks, ignore_index=True, sort=True)into a single dataframe\n",
    "    certsDF = pd.concat(chunks, ignore_index=True, sort=True)\n",
    "\n",
    "    timing.log(\"finding dups\")\n",
    "    dups = certsDF.duplicated(subset=\"leaf_cert.fingerprint\")\n",
    "    dups = certsDF[dups]\n",
    "    timing.log(\"writing dups to file\")\n",
    "    dups.to_csv(\"duplicates_readInOutIn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**post-hoc**\n",
    "\n",
    "I seriously considered not including this method, because I never actually tested it. As I was preparing to do so, it hit me that what I was trying to do was break the file into even smaller pieces than the chunks, but the size of the file wasn't really the issue. The problem stemmed from the amount of time it takes to _process_ each row.\n",
    "\n",
    "**ad-hoc**\n",
    "\n",
    "As I was thinking about getting readInOutIn to work, it hit me that I've been thinking about this a consecutive manner. There's only so far I can go with it. I need to think about serializing, ergo, THREADING! \n",
    "\n",
    "I know that reading in a file shouldn't be threaded, but given that that's not the part that takes forever (based on my previous attempts). That part is the processing the chunks (i.e. the normalizing) portion. So I can thread that. \n",
    "\n",
    "Just as I was about to add threading to my usingPANDAS framework, I decided to google adding threading to PANDAS. This lead to DASK! Rather than reinventing the wheel, I'm using it. https://docs.dask.org/en/latest/dataframe-api.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 5: usingDASK\n",
    "**post-hoc**\n",
    "\n",
    "The code that follows is so small comparied to the amount of time I spent trying to get it to work. There were a number of road-blocks, including having to define the column types (eventually I gave up being flexible and created the \"metas\" dictionary, which I didn't end up using), the nested jsons (again!) (I suddenly remembered my flattenDict method from days ago), and tuning the blocksize. I did have all of these adventures outlined, but eventually I found myself going in circles and spinning in the mud. I wiped the slate clean and started from scratch. As I'm sure you've experienced, I solved the problems within the hour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def usingDASK(file):\n",
    "    chunks = []\n",
    "    timing.log(\"Starting reading-in\")\n",
    "\n",
    "    reader = dd.read_json(\n",
    "        file,\n",
    "        lines=True,\n",
    "        blocksize=2 ** 24, #blocksize tuned using 1GB sample file\n",
    "        meta={\"data\": object, \"message_type\": object},\n",
    "    )\n",
    "  \n",
    "    datas = (\n",
    "        reader[\"data\"]\n",
    "        .map_partitions(lambda df: df.apply((lambda row: flattenDict(row, \"\", {}))))\n",
    "        .to_bag()\n",
    "    )\n",
    "    new = datas.to_dataframe()\n",
    "    new[\"message_type\"] = reader[\"message_type\"]\n",
    "    new = new.compute()\n",
    "    dups = new.duplicated(subset=\"leaf_cert.fingerprint\")\n",
    "    dups = new[dups]\n",
    "    dups.to_csv(\"duplicates_DASK.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "My poor laptop is still running the huge file with usingDASK, but my preliminary testing suggests that it is as much of an improvement in speed as I hope. There are still 30 minutes before it hits the 2 hour mark, but frankly, even if it goes over, it's 8:30pm the day before I have to give this to you. I'm not holding my breath that I'll be able to improve on the final time.\n",
    "\n",
    "If you were wondering, I did begin the bonus challenge while I was waiting for things to process. I didn't get anywhere near as far as I wanted, but if you're at all curious about my initial plan of attack, the script is 'bonusFindPhishing.py'. It gives a more realistic view of how my scripts are typically organized and commented than searchDuplicateCrts.py.\n",
    "\n",
    "I want to thank you for giving me this chance and for reading this to the end. I hope you find what you're looking for in me. If you have any questions, you know how to reach me.\n",
    "\n",
    "Sincerely,\n",
    "\n",
    "Emma Bailin\n",
    "\n",
    "Update (morning before submission): usingDASK wasn't as successful as I'd hoped. I'm suspicious that the blocksize I chose for usingDASK was too big. I tuned it using the 1GB file, but I don't think I took the expansion into account. Baring the blocksize tuning, I can see another avenue being using sqlite or something similar to do the loading, leveraging Dask's prep expertise and SQL's storage options. I'd like to explore these options, but I recognize I need to submit what I've got to you now. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
